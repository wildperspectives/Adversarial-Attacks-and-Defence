{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe5vvzF4HGAtoiY19NwE2U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaurav7888/Adversarial-Attacks-and-Defence/blob/main/Text_Based_Attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY_FVCb_6Vge"
      },
      "outputs": [],
      "source": [
        "!pip3 install textattack[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack peek-dataset --dataset-from-huggingface rotten_tomatoes\n"
      ],
      "metadata": {
        "id": "G5mSW8pX60eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack train \\\n",
        "    --model distilbert-base-uncased \\\n",
        "    --dataset rotten_tomatoes \\\n",
        "    --model-num-labels 2 \\\n",
        "    --model-max-length 64 \\\n",
        "    --per-device-train-batch-size 128 \\\n",
        "    --num-epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQJFe0eA63nO",
        "outputId": "e7424cec-a5b4-44d3-ca4e-26d6ddfcb5ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: distilbert-base-uncased\n",
            "Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 81.5kB/s]\n",
            "Downloading pytorch_model.bin: 100% 268M/268M [00:01<00:00, 196MB/s]\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 12.2kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 4.54MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 10.1MB/s]\n",
            "Using custom data configuration default\n",
            "Reusing dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
            "100% 3/3 [00:00<00:00, 620.61it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "Using custom data configuration default\n",
            "Reusing dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
            "100% 3/3 [00:00<00:00, 1001.74it/s]\n",
            "Using custom data configuration default\n",
            "Reusing dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
            "100% 3/3 [00:00<00:00, 852.33it/s]\n",
            "Using custom data configuration default\n",
            "Reusing dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
            "100% 3/3 [00:00<00:00, 976.93it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mvalidation\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Writing logs to ./outputs/2023-04-08-08-31-11-309154/train_log.txt.\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to ./outputs/2023-04-08-08-31-11-309154/training_args.json.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num examples = 8530\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num epochs = 5\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num clean epochs = 5\n",
            "\u001b[34;1mtextattack\u001b[0m:   Instantaneous batch size per device = 128\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "\u001b[34;1mtextattack\u001b[0m:   Gradient accumulation steps = 1\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total optimization steps = 335\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 1\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 1/5\n",
            "Loss 0.68037: 100% 67/67 [00:42<00:00,  1.59it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 58.30%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 76.17%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-04-08-08-31-11-309154/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 2\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 2/5\n",
            "Loss 0.56760: 100% 67/67 [00:42<00:00,  1.58it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 80.32%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 83.40%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-04-08-08-31-11-309154/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 3\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 3/5\n",
            "Loss 0.48775: 100% 67/67 [00:41<00:00,  1.60it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 86.19%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 85.46%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-04-08-08-31-11-309154/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 4\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 4/5\n",
            "Loss 0.42441: 100% 67/67 [00:42<00:00,  1.59it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 91.16%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 86.12%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-04-08-08-31-11-309154/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 5\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 5/5\n",
            "Loss 0.37018: 100% 67/67 [00:41<00:00,  1.60it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 94.84%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 84.80%\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote README to ./outputs/2023-04-08-08-31-11-309154/README.md.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack eval --num-examples 1000 --model ./outputs/2023-04-08-08-31-11-309154//best_model/ --dataset-from-huggingface rotten_tomatoes --dataset-split test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAARAuSO7EFm",
        "outputId": "ef4f9247-eb6c-418f-abf6-8e0a7eb0b735"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
            "\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 775.53it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Got 1000 predictions.\n",
            "\u001b[34;1mtextattack\u001b[0m: Correct 850/1000 (\u001b[94m85.00%\u001b[0m)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack attack --recipe textfooler --num-examples 20 --model ./outputs/2023-04-08-08-31-11-309154/best_model/ --dataset-from-huggingface rotten_tomatoes --dataset-split test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGCcf9EY7G-0",
        "outputId": "4d18d453-86b4-4b62-d5b4-d5ef4fe68396"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset rotten_tomatoes (/root/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
            "\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 745.30it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  WordSwapEmbedding(\n",
            "    (max_candidates):  50\n",
            "    (embedding):  WordEmbedding\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): WordEmbeddingDistance(\n",
            "        (embedding):  WordEmbedding\n",
            "        (min_cos_sim):  0.5\n",
            "        (cased):  False\n",
            "        (include_unknown_words):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): PartOfSpeech(\n",
            "        (tagger_type):  nltk\n",
            "        (tagset):  universal\n",
            "        (allow_verb_noun_swap):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (2): UniversalSentenceEncoder(\n",
            "        (metric):  angular\n",
            "        (threshold):  0.840845057\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  False\n",
            "      )\n",
            "    (3): RepeatModification\n",
            "    (4): StopwordModification\n",
            "    (5): InputColumnModification(\n",
            "        (matching_column_labels):  ['premise', 'hypothesis']\n",
            "        (columns_to_ignore):  {'premise'}\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n",
            "  0% 0/20 [00:00<?, ?it/s]2023-04-08 08:51:56.581423: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "  5% 1/20 [00:07<02:28,  7.81s/it]--------------------------------------------- Result 1 ---------------------------------------------\n",
            "\u001b[92mPositive (87%)\u001b[0m --> \u001b[91mNegative (50%)\u001b[0m\n",
            "\n",
            "lovingly photographed in the manner of a golden book sprung to life , stuart little 2 \u001b[92mmanages\u001b[0m sweetness largely without stickiness .\n",
            "\n",
            "lovingly photographed in the manner of a golden book sprung to life , stuart little 2 \u001b[91mexecutive\u001b[0m sweetness largely without stickiness .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10% 2/20 [00:08<01:13,  4.09s/it]--------------------------------------------- Result 2 ---------------------------------------------\n",
            "\u001b[92mPositive (99%)\u001b[0m --> \u001b[91mNegative (95%)\u001b[0m\n",
            "\n",
            "consistently \u001b[92mclever\u001b[0m and \u001b[92msuspenseful\u001b[0m .\n",
            "\n",
            "consistently \u001b[91mmalin\u001b[0m and \u001b[91menigmatic\u001b[0m .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  10% 2/20 [00:08<01:13,  4.10s/it]--------------------------------------------- Result 3 ---------------------------------------------\n",
            "\u001b[91mNegative (84%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 2 / 0 / 1 / 3:  20% 4/20 [00:08<00:34,  2.13s/it]--------------------------------------------- Result 4 ---------------------------------------------\n",
            "\u001b[92mPositive (98%)\u001b[0m --> \u001b[91mNegative (67%)\u001b[0m\n",
            "\n",
            "the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with \u001b[92mtremendous\u001b[0m \u001b[92mskill\u001b[0m .\n",
            "\n",
            "the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with \u001b[91mmassive\u001b[0m \u001b[91mjurisdictional\u001b[0m .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 3 / 0 / 1 / 4:  20% 4/20 [00:08<00:34,  2.13s/it]--------------------------------------------- Result 5 ---------------------------------------------\n",
            "\u001b[92mPositive (50%)\u001b[0m --> \u001b[91mNegative (85%)\u001b[0m\n",
            "\n",
            "red dragon \" never \u001b[92mcuts\u001b[0m corners .\n",
            "\n",
            "red dragon \" never \u001b[91mclipping\u001b[0m corners .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 4 / 0 / 1 / 5:  30% 6/20 [00:08<00:20,  1.44s/it]--------------------------------------------- Result 6 ---------------------------------------------\n",
            "\u001b[91mNegative (70%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 4 / 0 / 2 / 6:  30% 6/20 [00:08<00:20,  1.44s/it]--------------------------------------------- Result 7 ---------------------------------------------\n",
            "\u001b[92mPositive (98%)\u001b[0m --> \u001b[91mNegative (73%)\u001b[0m\n",
            "\n",
            "throws in enough clever and \u001b[92munexpected\u001b[0m \u001b[92mtwists\u001b[0m to make the formula feel fresh .\n",
            "\n",
            "throws in enough clever and \u001b[91munwanted\u001b[0m \u001b[91mtendrils\u001b[0m to make the formula feel fresh .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 5 / 0 / 2 / 7:  40% 8/20 [00:08<00:13,  1.10s/it]--------------------------------------------- Result 8 ---------------------------------------------\n",
            "\u001b[91mNegative (83%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "weighty and ponderous but every bit as filling as the treat of the title .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 5 / 0 / 3 / 8:  40% 8/20 [00:08<00:13,  1.10s/it]--------------------------------------------- Result 9 ---------------------------------------------\n",
            "\u001b[92mPositive (98%)\u001b[0m --> \u001b[91mNegative (51%)\u001b[0m\n",
            "\n",
            "a real audience-pleaser that will \u001b[92mstrike\u001b[0m a \u001b[92mchord\u001b[0m with anyone who's ever \u001b[92mwaited\u001b[0m in a doctor's office , emergency room , \u001b[92mhospital\u001b[0m \u001b[92mbed\u001b[0m or insurance company office .\n",
            "\n",
            "a real audience-pleaser that will \u001b[91mslugged\u001b[0m a \u001b[91mchords\u001b[0m with anyone who's ever \u001b[91mplanned\u001b[0m in a doctor's office , emergency room , \u001b[91mcommittal\u001b[0m \u001b[91mnapping\u001b[0m or insurance company office .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 6 / 0 / 3 / 9:  50% 10/20 [00:09<00:09,  1.02it/s]--------------------------------------------- Result 10 ---------------------------------------------\n",
            "\u001b[92mPositive (99%)\u001b[0m --> \u001b[91mNegative (77%)\u001b[0m\n",
            "\n",
            "generates an enormous \u001b[92mfeeling\u001b[0m of empathy for its \u001b[92mcharacters\u001b[0m .\n",
            "\n",
            "generates an enormous \u001b[91mpremonition\u001b[0m of empathy for its \u001b[91mfont\u001b[0m .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 7 / 0 / 3 / 10:  50% 10/20 [00:09<00:09,  1.02it/s]--------------------------------------------- Result 11 ---------------------------------------------\n",
            "\u001b[92mPositive (99%)\u001b[0m --> \u001b[91mNegative (82%)\u001b[0m\n",
            "\n",
            "exposing the ways we fool ourselves is one hour photo's real \u001b[92mstrength\u001b[0m .\n",
            "\n",
            "exposing the ways we fool ourselves is one hour photo's real \u001b[91mharshness\u001b[0m .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 8 / 0 / 3 / 11:  60% 12/20 [00:10<00:06,  1.20it/s]--------------------------------------------- Result 12 ---------------------------------------------\n",
            "\u001b[92mPositive (75%)\u001b[0m --> \u001b[91mNegative (53%)\u001b[0m\n",
            "\n",
            "it's up to you to decide whether to admire these people's dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful \u001b[92mview\u001b[0m of american life .\n",
            "\n",
            "it's up to you to decide whether to admire these people's dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful \u001b[91mconsults\u001b[0m of american life .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 9 / 0 / 3 / 12:  60% 12/20 [00:10<00:06,  1.20it/s]--------------------------------------------- Result 13 ---------------------------------------------\n",
            "\u001b[91mNegative (53%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 9 / 0 / 4 / 13:  70% 14/20 [00:10<00:04,  1.38it/s]--------------------------------------------- Result 14 ---------------------------------------------\n",
            "\u001b[92mPositive (95%)\u001b[0m --> \u001b[91mNegative (53%)\u001b[0m\n",
            "\n",
            ". . . quite \u001b[92mgood\u001b[0m at providing some good old \u001b[92mfashioned\u001b[0m spooks .\n",
            "\n",
            ". . . quite \u001b[91mbestest\u001b[0m at providing some good old \u001b[91mmoulded\u001b[0m spooks .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 10 / 0 / 4 / 14:  70% 14/20 [00:10<00:04,  1.38it/s]--------------------------------------------- Result 15 ---------------------------------------------\n",
            "\u001b[91mNegative (90%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 10 / 0 / 5 / 15:  80% 16/20 [00:10<00:02,  1.53it/s]--------------------------------------------- Result 16 ---------------------------------------------\n",
            "\u001b[92mPositive (99%)\u001b[0m --> \u001b[91mNegative (53%)\u001b[0m\n",
            "\n",
            "scherfig's light-hearted \u001b[92mprofile\u001b[0m of emotional desperation is achingly \u001b[92mhonest\u001b[0m and \u001b[92mdelightfully\u001b[0m cheeky .\n",
            "\n",
            "scherfig's light-hearted \u001b[91mdescribe\u001b[0m of emotional desperation is achingly \u001b[91mfranco\u001b[0m and \u001b[91mblithely\u001b[0m cheeky .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 11 / 0 / 5 / 16:  80% 16/20 [00:10<00:02,  1.53it/s]--------------------------------------------- Result 17 ---------------------------------------------\n",
            "\u001b[92mPositive (99%)\u001b[0m --> \u001b[91mNegative (52%)\u001b[0m\n",
            "\n",
            "a journey \u001b[92mspanning\u001b[0m nearly three decades of bittersweet \u001b[92mcamaraderie\u001b[0m and history , in which we \u001b[92mfeel\u001b[0m that we truly know what makes holly and marina tick , and our \u001b[92mhearts\u001b[0m go out to them as both \u001b[92mcontinue\u001b[0m to negotiate their imperfect , love-hate \u001b[92mrelationship\u001b[0m .\n",
            "\n",
            "a journey \u001b[91mexpectancy\u001b[0m nearly three decades of bittersweet \u001b[91mcollage\u001b[0m and history , in which we \u001b[91mpresume\u001b[0m that we truly know what makes holly and marina tick , and our \u001b[91mcardiology\u001b[0m go out to them as both \u001b[91mceaseless\u001b[0m to negotiate their imperfect , love-hate \u001b[91mreports\u001b[0m .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 12 / 0 / 5 / 17:  90% 18/20 [00:11<00:01,  1.63it/s]--------------------------------------------- Result 18 ---------------------------------------------\n",
            "\u001b[91mNegative (62%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "the wonderfully lush morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh's book trainspotting .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 12 / 0 / 6 / 18:  90% 18/20 [00:11<00:01,  1.63it/s]--------------------------------------------- Result 19 ---------------------------------------------\n",
            "\u001b[92mPositive (60%)\u001b[0m --> \u001b[91mNegative (78%)\u001b[0m\n",
            "\n",
            "as it \u001b[92mturns\u001b[0m out , you can go \u001b[92mhome\u001b[0m again .\n",
            "\n",
            "as it \u001b[91mconverting\u001b[0m out , you can go \u001b[91mhabitation\u001b[0m again .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 13 / 0 / 6 / 19: 100% 20/20 [00:11<00:00,  1.77it/s]--------------------------------------------- Result 20 ---------------------------------------------\n",
            "\u001b[92mPositive (94%)\u001b[0m --> \u001b[91mNegative (79%)\u001b[0m\n",
            "\n",
            "you've already seen city by the sea under a variety of titles , but it's \u001b[92mworth\u001b[0m yet another visit .\n",
            "\n",
            "you've already seen city by the sea under a variety of titles , but it's \u001b[91mchastisement\u001b[0m yet another visit .\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 14 / 0 / 6 / 20: 100% 20/20 [00:11<00:00,  1.77it/s]\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 14     |\n",
            "| Number of failed attacks:     | 0      |\n",
            "| Number of skipped attacks:    | 6      |\n",
            "| Original accuracy:            | 70.0%  |\n",
            "| Accuracy under attack:        | 0.0%   |\n",
            "| Attack success rate:          | 100.0% |\n",
            "| Average perturbed word %:     | 17.51% |\n",
            "| Average num. words per input: | 17.5   |\n",
            "| Avg num queries:              | 75.86  |\n",
            "+-------------------------------+--------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eq30HSey7ynb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}